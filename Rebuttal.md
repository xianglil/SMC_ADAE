## The SMC-NPU Algorithm
Note that SMC-GPU are designed for the SIMT programming model in GPU. The vector unit in Ascend NPU adopts the SIMD programming model.
#### Motivation 
The NPUs are specifically designed for deep learning tasks. Compared to SIMD vectorized instruction sets on CPUs, the SIMD computation of the Ascend chip's vector processing unit has another notable characteristic: data reads and writes must be memory-contiguous or segment-contiguous, which poses significant programming challenges for general HPC tasks. Note that both CPUs and GPUs can gather and assemble non-contiguous scalar data, whereas the Ascend chip cannot.

![alt text](image.png)

Specifically, Monte Carlo simulations generally exhibit random memory access patterns. For simplicity, we use a 2D square lattice for discussion, as shown in Fig.1. In an atom-swap trial in SMC, while the red atoms follow a regular pattern, their exchange neighbors (orange atoms) are randomly distributed in space. In CPU and GPU architectures, each red atom attempts to swap randomly with its nearest neighbor. Each CPU core or GPU thread executes identical exchange instructions while reading atom-specific data from different memory locations - an operation that poses little challenges for conventional architectures, but presents serious computational bottleneck for NPUs due to the abscence of real memory cache system.

To address the above problem, we propose the SMC-NPU algorithm, which is designed to unleash the computing power in NPU for MCMC atomistic simulation by **coalescing memory access pattern, as much as possible, and vectorize the scalar operations via masked operations**. We will explain the two optimization techniques in detail in the following.

#### Memory Access Coalescing
In SMC-GPU, only one lattice are used, and all swap trials are attempted on-site. While such a practice saves memory space, it inevitablly introduces frequent read and write operations that demand irregular memory access, which is highly unfavorable for NPU.
![alt text](image-1.png)
To coalescing the memory access, the SMC-NPU algorithm uses **two lattices** to record the atomic configuratoins before and after the swap trials, respectively, as shown in Fig. 6 of the manuscript. Note that the usage of of two lattices enables the **decoupling of the two important steps: energy calculation, and Metropolis updating**, as illustrated in Fig. 5. As a result, the most computing-intensive step, the calculation of the local energies for each sites, can be done in an **embarassively-parallel way, with contigious memory access**. While some overhead still exists due to the need to padding the neighbor vectors for the hardware's SIMD width, the contigious memory access pattern, as well as the huge parallel degrees introduced by SMC-NPU, can effectively utilize the large number (20) of 2048-bit SIMD vector units in an NPU. Note that the lattice are stored as Int16, therefere a total of 128*20=2560 sites can be processed simultaneously in a clock cycle by a single  **TBD with Kai**. 
![alt text](image-2.png)
We highlight that, unlike the NPU-specific masked-operation technique to be discussed below, the above optimization technique in principle can be applied to any vector-based accelerators, in additional to NPUs, to convert irregular memory access in atomistic simulation to regular ones, at the cost of doubling the memory usage for storing the lattice.
#### Vectorization via Masked Operations
The vector processing unit supports a select operation, which performs an element-wise selection between two source operand vectors and stores the result in a destination vector. The selection is mask-controlled:
![alt text](image-3.png)
* When a mask bit is 1, the corresponding element is taken from the first source operand vector.

* When a mask bit is 0, the element is selected from the second source operand vector.

As mentioned, due to the limit of the programming model in NPU, the conditional branching vector operations and scalar operations need to be re-implemented as masked vector operation in NPU, as illustrated in the green boxes in Fig. 5. Note that some masks are generated by multiple steps with other auxillary quantities. For instance, the neighbor-swap mask in the second green box in Fig. 5 is obtained as: 
1. Calculate a swap-trial label vector $v_0$, from the random numbers. Each of the element $i$ in $v_0$ can take a value between 0 and $N_{nn}-1$, which is the number of nearest neighbor. (e.g. $N_{nn} = 4$ for the 2D square lattice in Fig. 6). E.g.![alt text](image-4.png)
2. A mask $m_1$ to select the red atoms in Fig. 1. E.g. ![alt text](image-5.png)
3. Calculate $N_{nn}$ mask $m_2^i$ for each of the nearest-neighbor site, on the fly, from $v_0$ in step 1. E.g. $m_2^0$:![alt text](image-6.png)
4. For each mask $m_2^i$, do a bit-wise `AND` operation with $m_1$ to obtain $m_3^i$. E.g. $m_3^0$: 
![alt text](image-7.png), which represents that the site labeled as 1 need to swap with the 0-th neighbor (well, zero-th neighor sounds a little strange, but I suppose the readers understood what I meant).
5. Use the $N_{NN}$ masks $m_3^i$ to construct the swaped lattice vectors.

Its obvious that the above steps involves many vector operations. The total number of operations in a MC sweep is approximately proportional to $N*L_{LC}*N_{NN}$, where $N$ is the number of atoms in the lattice, $L_{LC}$ is the length of the link cell. Similarly, for the reduce operation of the local energies to obtain block_energy, the total number of operation in a MC sweep is approximately $N*N_{LIZ}$, where $N_{LIZ}$ is the size of the local interaction zone.

#### Ghost Boundary
Another key difference between SMC-NPU and SMC-GPU is on handling the boundary condition for a lattice distributed over multiple GPUs.

#### Performance Analysis